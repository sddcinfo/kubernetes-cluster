# Kubespray Integration Plan for Proxmox Infrastructure

## Executive Summary

Integration of Kubespray v2.28.1+ with the existing Proxmox-based kubernetes-cluster automation to replace custom Ansible roles while maintaining the current orchestration framework.

## Current State vs Target State

### Current Architecture
- **Custom Ansible Roles**: Manual kubeadm-based cluster initialization
- **Kubernetes Version**: 1.29 (manually managed)
- **CNI**: Custom Cilium setup
- **Infrastructure**: OpenTofu + Proxmox provider
- **Orchestration**: Python scripts (cluster-manager.py, cluster-deploy.py)

### Target Architecture with Kubespray
- **Kubespray Integration**: Replace custom roles with Kubespray v2.28.1+
- **Kubernetes Version**: 1.33.4 (supported by latest Kubespray)
- **CNI**: Cilium 1.17.7 → upgrade to 1.18.1 (via Kubespray override)
- **Infrastructure**: Keep existing OpenTofu + Proxmox
- **Orchestration**: Enhanced Python scripts with Kubespray workflow

## Version Compatibility Analysis

### Kubespray v2.28.1 Support
- ✅ **Kubernetes 1.33.4**: Fully supported (found in kubelet_checksums)
- ⚠️ **Cilium 1.18.1**: Default is 1.17.7, needs override to 1.18.1
- ✅ **Containerd**: Native support with systemd cgroup
- ✅ **Ubuntu 24.04**: Supported distribution
- ✅ **Proxmox**: Infrastructure agnostic (works with any VM provider)

### Required Overrides
```yaml
# group_vars/k8s_cluster/k8s-cluster.yml
kube_version: "1.33.4"

# group_vars/k8s_cluster/k8s-net-cilium.yml
cilium_version: "1.18.1"
```

## Integration Strategy

### Phase 1: Setup Kubespray Infrastructure
1. **Clone Kubespray** into kubernetes-cluster/kubespray/
2. **Create Inventory Template** for Proxmox-generated VMs
3. **Configure Group Variables** with Proxmox-specific settings
4. **Test Kubespray Setup** on existing infrastructure

### Phase 2: Replace Custom Roles
1. **Remove Custom Ansible Roles**:
   - `ansible/roles/common/`
   - `ansible/roles/kubernetes/`
   - `ansible/roles/kubernetes_init/`
   - `ansible/roles/kubernetes_join_*/`
   - `ansible/roles/kubernetes_post_install/`

2. **Create Kubespray Wrapper Playbook**:
   - `ansible/kubespray-deploy.yml`
   - Integrates with existing inventory from OpenTofu

### Phase 3: Orchestration Integration
1. **Update cluster-manager.py**: Add Kubespray setup phase
2. **Update cluster-deploy.py**: Replace Ansible calls with Kubespray
3. **Maintain DNS Configuration**: Keep existing DNS automation
4. **Preserve Monitoring Stack**: Keep platform services deployment

### Phase 4: Testing and Validation
1. **End-to-End Testing**: Full cluster deployment
2. **Feature Validation**: Ensure all current features work
3. **Performance Testing**: Verify no regression in deployment time
4. **Documentation Update**: Update all guides and README

## Implementation Details

### Directory Structure Changes
```
kubernetes-cluster/
├── kubespray/                    # NEW: Kubespray submodule/clone
│   ├── inventory/
│   │   └── proxmox-cluster/      # NEW: Our inventory
│   └── ...                       # Standard Kubespray structure
├── ansible/
│   ├── kubespray-deploy.yml      # NEW: Wrapper playbook
│   ├── inventory.yml             # MODIFIED: Generated by OpenTofu
│   └── roles/                    # REMOVED: Custom roles deleted
├── scripts/
│   ├── cluster-manager.py        # MODIFIED: Add Kubespray setup
│   └── cluster-deploy.py         # MODIFIED: Call Kubespray instead of custom roles
└── terraform/                    # UNCHANGED: Keep existing infrastructure
```

### Kubespray Configuration Files

#### inventory/proxmox-cluster/inventory.ini
```ini
[all]
k8s-control-1 ansible_host=10.10.1.31 ip=10.10.1.31
k8s-control-2 ansible_host=10.10.1.32 ip=10.10.1.32
k8s-control-3 ansible_host=10.10.1.33 ip=10.10.1.33
k8s-worker-1 ansible_host=10.10.1.40 ip=10.10.1.40
k8s-worker-2 ansible_host=10.10.1.41 ip=10.10.1.41
k8s-worker-3 ansible_host=10.10.1.42 ip=10.10.1.42
k8s-worker-4 ansible_host=10.10.1.43 ip=10.10.1.43

[kube_control_plane]
k8s-control-1
k8s-control-2
k8s-control-3

[etcd]
k8s-control-1
k8s-control-2
k8s-control-3

[kube_node]
k8s-worker-1
k8s-worker-2
k8s-worker-3
k8s-worker-4

[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr
```

#### group_vars/k8s_cluster/k8s-cluster.yml
```yaml
---
# Kubernetes version
kube_version: "1.33.4"

# Networking
kube_pods_subnet: 10.244.0.0/16
kube_service_addresses: 10.96.0.0/12
kube_network_plugin: cilium

# Proxmox-specific
ansible_user: ubuntu
ansible_ssh_private_key_file: /home/sysadmin/.ssh/sysladmin_automation_key

# High Availability
kube_apiserver_address: "{{ hostvars['k8s-control-1']['ip'] | default(hostvars['k8s-control-1']['ansible_default_ipv4']['address']) }}"
kube_apiserver_port: 6443

# Container runtime
container_manager: containerd
```

#### group_vars/k8s_cluster/k8s-net-cilium.yml
```yaml
---
# Cilium configuration
cilium_version: "1.18.1"
cilium_mtu: 1500
cilium_enable_ipv4: true
cilium_enable_ipv6: false
cilium_identity_allocation_mode: crd

# eBPF features
cilium_enable_bpf_masquerade: true
cilium_enable_host_reachable_services: true

# Integration with MetalLB
cilium_l2announcements: false  # Keep MetalLB for LoadBalancer services
```

### Orchestration Script Changes

#### cluster-manager.py additions:
```python
def setup_kubespray():
    """Setup Kubespray for cluster deployment"""
    kubespray_dir = Path("kubespray")
    
    if not kubespray_dir.exists():
        print("🔄 Cloning Kubespray...")
        subprocess.run([
            "git", "clone", 
            "https://github.com/kubernetes-sigs/kubespray.git",
            str(kubespray_dir)
        ], check=True)
    
    print("🔧 Setting up Kubespray inventory...")
    inventory_dir = kubespray_dir / "inventory" / "proxmox-cluster"
    inventory_dir.mkdir(parents=True, exist_ok=True)
    
    # Copy sample configurations
    shutil.copytree(
        kubespray_dir / "inventory" / "sample" / "group_vars",
        inventory_dir / "group_vars",
        dirs_exist_ok=True
    )
    
    # Generate inventory from OpenTofu output
    generate_kubespray_inventory()
    
    print("✅ Kubespray setup complete")
```

#### cluster-deploy.py modifications:
```python
def deploy_kubernetes_cluster():
    """Deploy Kubernetes cluster using Kubespray"""
    os.chdir("kubespray")
    
    # Install Kubespray requirements
    subprocess.run([
        "pip3", "install", "-r", "requirements.txt"
    ], check=True)
    
    # Deploy cluster
    subprocess.run([
        "ansible-playbook", 
        "-i", "inventory/proxmox-cluster/inventory.ini",
        "cluster.yml",
        "-b"
    ], check=True)
    
    # Copy kubeconfig
    copy_kubeconfig_for_access()
    
    print("✅ Kubernetes cluster deployed successfully")
```

## Migration Benefits

### Advantages
1. **Reduced Maintenance**: Community-maintained Kubernetes deployment
2. **Latest Features**: Always up-to-date with Kubernetes releases
3. **Battle-Tested**: Production-proven configurations
4. **Advanced CNI Support**: Better Cilium integration and features
5. **Comprehensive Addons**: Built-in support for monitoring, ingress, etc.

### Risk Mitigation
1. **Gradual Migration**: Phase-based approach with testing at each step
2. **Rollback Plan**: Keep custom roles in git history
3. **Feature Parity**: Validate all current features work
4. **Infrastructure Unchanged**: No changes to proven OpenTofu setup

## Timeline Estimate

- **Phase 1**: 1 day (setup and configuration)
- **Phase 2**: 1 day (role replacement)
- **Phase 3**: 1 day (orchestration updates)
- **Phase 4**: 1 day (testing and validation)

**Total**: 4 days for complete migration

## Success Criteria

1. ✅ **Deployment Time**: < 20 minutes for full cluster
2. ✅ **Feature Parity**: All current features working
3. ✅ **Version Target**: Kubernetes 1.33.4 + Cilium 1.18.1
4. ✅ **Idempotency**: Safe to run multiple times
5. ✅ **Documentation**: Updated and comprehensive

## Next Steps

1. **Begin Phase 1**: Setup Kubespray in feature branch
2. **Test on Development**: Validate approach on test environment
3. **Gradual Rollout**: Phase-by-phase implementation
4. **Production Deployment**: Final validation and cutover

---

*This plan maintains the best aspects of the current architecture while leveraging the proven capabilities of Kubespray for Kubernetes cluster management.*